# üèÄ Analisis Predictivo de Talento NBA
<img width="1024" height="1024" alt="NBA" src="https://github.com/user-attachments/assets/f03a3e33-3678-42a9-8b86-ddc6481043fc" />

# üìå Contexto del proyecto

El baloncesto profesional (NBA) utiliza m√©tricas f√≠sicas obtenidas en el Draft Combine para evaluar el potencial de nuevos jugadores antes de invertir millones de d√≥lares en contratos. Sin embargo, existe una alta incertidumbre sobre si estas m√©tricas f√≠sicas (salto, velocidad, agilidad, envergadura) realmente predicen el rendimiento profesional real en cancha.
Este proyecto busca reducir el riesgo de draft busts mediante un enfoque de Data Analytics y Data Science, integrando datos f√≠sicos pre-draft con estad√≠sticas hist√≥ricas de juego.



# üéØ  Objetivo general

Analizar la relaci√≥n entre las m√©tricas f√≠sicas del Draft Combine y el rendimiento real en la NBA, construyendo una base anal√≠tica s√≥lida que permita:

Identificar perfiles f√≠sicos con mayor probabilidad de √©xito.

Comparar el impacto f√≠sico seg√∫n posici√≥n.

Servir como base para an√°lisis estad√≠stico, modelos predictivos y dashboards.


#  üß± Alcance del Sprint #1

Durante este primer sprint se trabaj√≥ exclusivamente en la ingenier√≠a de datos y modelado, dejando preparada la infraestructura para los an√°lisis posteriores.

Las tareas realizadas fueron:

Exploraci√≥n, limpieza y normalizaci√≥n de datos en Python.

Dise√±o del modelo dimensional (esquema estrella).

Creaci√≥n de un Data Warehouse en SQL Server.

Preparaci√≥n de datos listos para an√°lisis y visualizaci√≥n.


#  üìä Datos utilizados

Se trabaj√≥ con el dataset NBA Database (Kaggle), compuesto por:

Base SQLite hist√≥rica (desde 1946).

Tablas clave:

common_player_info ‚Üí informaci√≥n biogr√°fica y de carrera.

draft_combine_stats ‚Üí m√©tricas f√≠sicas pre-draft.

game y other_stats ‚Üí estad√≠sticas de partidos a nivel equipo.



# üßπ Limpieza y normalizaci√≥n de datos (Python)

La limpieza y transformaci√≥n se realiz√≥ √≠ntegramente en Python (Pandas + NumPy), antes de cualquier carga a SQL, siguiendo buenas pr√°cticas de ETL.

Principales decisiones t√©cnicas:

Normalizaci√≥n de unidades:

Altura: de formato texto (pies-pulgadas) a cent√≠metros.

Peso: de libras a kilogramos.

Conversi√≥n de tipos:

M√©tricas f√≠sicas y estad√≠sticas convertidas a tipos num√©ricos.

Gesti√≥n de nulos:

No se imputaron valores f√≠sicos faltantes para evitar sesgos.

Se filtraron registros inv√°lidos solo cuando afectaban claves anal√≠ticas.

Estandarizaci√≥n sem√°ntica:

Normalizaci√≥n de nombres, posiciones y claves de jugadores/equipos.

Eliminaci√≥n de duplicados:

Garantizando unicidad por identificadores naturales (person_id, team_id, game_id).

Este enfoque asegura calidad anal√≠tica, trazabilidad y reproducibilidad.





# ‚≠ê Dise√±o del modelo dimensional (Esquema Estrella)

Se dise√±√≥ un modelo estrella orientado a an√°lisis, priorizando simplicidad, escalabilidad y claridad sem√°ntica.

¬øPor qu√© dos tablas de hechos?

El proyecto aborda dos fen√≥menos distintos pero relacionados:

Rendimiento f√≠sico pre-draft

Rendimiento deportivo real en partidos

Forzar ambos conceptos en una √∫nica tabla hubiese generado:

Gran cantidad de nulos.

Ambig√ºedad en el grano de an√°lisis.

Dificultad para interpretar resultados.

Por ello, se opt√≥ por dos tablas de hechos.

# üü¶ Tablas de hechos FACT_COMBINE

Grano: Jugador ‚Äì Temporada

Contiene:

Salto vertical

Envergadura

Agilidad

Velocidad

Fuerza

Representa el potencial f√≠sico medido antes del ingreso a la NBA.

# FACT_GAME

Grano: Partido

M√©tricas a nivel equipo (home / away):

Puntos

Asistencias

P√©rdidas

Robos

Tapones

Plus/Minus

Representa el impacto real en cancha.

#  üü© Tablas de dimensiones

DIM_PLAYER: datos biogr√°ficos y f√≠sicos normalizados del jugador.

DIM_TEAM: informaci√≥n de franquicias NBA.

DIM_DATE: dimensi√≥n temporal para an√°lisis hist√≥ricos.

MAP_PLAYER_TEAM (tabla puente):

Resuelve la relaci√≥n muchos-a-muchos entre jugadores y equipos.

Mantiene la integridad hist√≥rica (from_year / to_year).


# üóÑÔ∏è Data Warehouse en SQL Server

Se implement√≥ un Data Warehouse en SQL Server con el objetivo de:

Centralizar datos limpios y normalizados.

Facilitar consultas anal√≠ticas complejas.

Servir como fuente confiable para Power BI y an√°lisis estad√≠stico.

Decisiones clave:

Uso de claves surrogate en dimensiones.

Separaci√≥n clara entre staging (Python) y almacenamiento anal√≠tico (SQL).

Dise√±o optimizado para joins, filtros y agregaciones.

SQL Server se utiliz√≥ exclusivamente como capa de almacenamiento anal√≠tico, evitando l√≥gica de limpieza dentro de la base.


# üõ†Ô∏è Tecnolog√≠as utilizadas

Python: Pandas, NumPy, SQLite3

Google Colab: ejecuci√≥n colaborativa y ETL

Google Drive: versionado y compartici√≥n de datos limpios

SQL Server: Data Warehouse

GitHub: control de versiones y documentaci√≥n
<img width="1024" height="1024" alt="tecnologias" src="https://github.com/user-attachments/assets/adbcc56f-0848-4d44-8114-348be2dc7dda" />


# Visualizaciones 


# üìå Conclusi√≥n

El Sprint #1 estableci√≥ una base s√≥lida de ingenier√≠a de datos, asegurando que cualquier an√°lisis posterior se realice sobre datos confiables, interpretables y bien modelados.
La arquitectura dise√±ada permite escalar el proyecto hacia an√°lisis avanzados y soporta decisiones reales de scouting deportivo basadas en datos.

## üë• Equipo
- Candela Paula Antognoli - Data Analyst
- Faber Garcia - Data Analyst
- Mauricio Veira Martinez - Data Analyst
- Manuel Arce- Data Analyst
